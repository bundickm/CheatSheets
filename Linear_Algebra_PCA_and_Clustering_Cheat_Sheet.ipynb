{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Algebra Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/Linear_Algebra_PCA_and_Clustering_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9p56FSWj2Kc",
        "colab_type": "text"
      },
      "source": [
        "#Resources\n",
        "[Numpy Linear Algebra Documentation](https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.linalg.html)\n",
        "\n",
        "[LaTex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxXVcLmllQjU",
        "colab_type": "text"
      },
      "source": [
        "#Basic Linear Algebra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuZ0KE7clfIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFnd8UfylLNl",
        "colab_type": "text"
      },
      "source": [
        "##Scalars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzKTJLGTnBz",
        "colab_type": "text"
      },
      "source": [
        "###Definition\n",
        "A single number. Variables representing scalars are typically written in lower case.\n",
        "\n",
        "Scalars can be whole numbers or decimals.\n",
        "\n",
        "\\begin{align}\n",
        "a = 2\n",
        "\\qquad\n",
        "b = 4.815162342\n",
        "\\end{align}\n",
        "\n",
        "They can be positive, negative, 0 or any other real number.\n",
        "\n",
        "\\begin{align}\n",
        "c = -6.022\\mathrm{e}{+23}\n",
        "\\qquad\n",
        "d = \\pi\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51IDwjHejk7Q",
        "colab_type": "code",
        "outputId": "4615e0a8-da79-498e-c830-3db0b552ac9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "scalar = 5\n",
        "vector = [1,2]\n",
        "matrix = [[1,2],\n",
        "          [3,4]]\n",
        "\n",
        "print('Scalar-Vector Multiplication:',np.multiply(scalar, vector))\n",
        "print('Scalar-Matrix Multiplication:\\n',np.multiply(scalar, matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scalar-Vector Multiplication: [ 5 10]\n",
            "Scalar-Matrix Multiplication:\n",
            " [[ 5 10]\n",
            " [15 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUFWRe7wmPBW",
        "colab_type": "text"
      },
      "source": [
        "##Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRhtzFMwTz0Z",
        "colab_type": "text"
      },
      "source": [
        "###Definition\n",
        "A vector of dimension *n* is an **ordered** collection of *n* elements, which are called **components** (Note, the components of a vector are **not** referred to as \"scalars\"). Vector notation variables are commonly written as a bold-faced lowercase letters or italicized non-bold-faced lowercase characters with an arrow (→) above the letters:\n",
        "\n",
        "Written: $\\vec{v}$ \n",
        "\n",
        "Examples:\n",
        "\n",
        "\\begin{align}\n",
        "   \\vec{a} = \n",
        "   \\begin{bmatrix}\n",
        "           1\\\\\n",
        "           2\n",
        "    \\end{bmatrix}\n",
        "    \\qquad\n",
        "    \\vec{b} =\n",
        "    \\begin{bmatrix}\n",
        "          -1\\\\\n",
        "           0\\\\\n",
        "           2\n",
        "    \\end{bmatrix}\n",
        "    \\qquad\n",
        "    \\vec{c} =\n",
        "    \\begin{bmatrix}\n",
        "           4.5\n",
        "    \\end{bmatrix}\n",
        "    \\qquad\n",
        "    \\vec{d} =\n",
        "    \\begin{bmatrix}\n",
        "           Pl\\\\\n",
        "           a\\\\\n",
        "           b\\\\\n",
        "           \\frac{2}{3}\n",
        "    \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "The above vectors have dimensions 2, 3, 1, and 4 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzg99dFw-Oj8",
        "colab_type": "text"
      },
      "source": [
        "### Norm of a Vector (Magnitude or length)\n",
        "\n",
        "The *Norm* or *Magnitude* of a vector is  the **length** of the vector. Since a vector is basically a line, if you treat it as the hypotenuse of a triangle you can use the pythagorean theorem to find the equation for the norm of a vector. We're essentially just generalizing the equation for the hypotenuse of a triangle that results from the pythagorean theorem to n dimensional space.\n",
        "\n",
        "We denote the norm of a vector by wrapping it in double pipes. The Norm is the square root of the sum of the squared elements of a vector.\n",
        "\\begin{align}\n",
        "||v|| = \n",
        "\\sqrt{v_{1}^2 + v_{2}^2 + \\ldots + v_{n}^2}\n",
        "\\\\\n",
        "\\vec{a} = \n",
        "\\begin{bmatrix}\n",
        "3 & 7 & 2 & 4\n",
        "\\end{bmatrix}\n",
        "\\\\\n",
        "||a|| = \\sqrt{3^2 + 7^2 + 2^2 + 4^2} \\\\\n",
        "||a|| = \\sqrt{9 + 49 + 4 + 16} \\\\\n",
        "||a|| = \\sqrt{78}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Properties of the Norm:\n",
        "\n",
        "* The norm is always positive or zero $||x|| \\geq 0$ \n",
        "\n",
        "* The norm is only equal to zero if all of the elements of the vector are zero.\n",
        "\n",
        "* The Triangle Inequality: $|| x + y ||\\leq ||x|| + ||y||$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAe6qIzml3go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_norm(vector):\n",
        "  return (sum([vector[i]**2 for i in range(len(vector))])**.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOr0BCxn-Wlh",
        "colab_type": "text"
      },
      "source": [
        "### Dot Product\n",
        "\n",
        "The dot product of two vectors $\\vec{a}$ and $\\vec{b}$ is a scalar quantity that is equal to the sum of pair-wise products of the components of vectors a and b. An example will make this make much more sense:\n",
        "\n",
        "\\begin{align} \\vec{a} \\cdot \\vec{b} = (a_{1} \\times b_{1}) + (a_{2} \\times b_{2}) + \\ldots + ( a_{n} \\times b_{n}) \\end{align}\n",
        "\n",
        "Example:\n",
        "\n",
        "\\begin{align}\n",
        "\\vec{a} = \n",
        "\\begin{bmatrix}\n",
        "3 & 7 & 2 & 4\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "\\vec{b} = \n",
        "\\begin{bmatrix}\n",
        "4 & 1 & 12 & 6\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "The dot product of two vectors would be:\n",
        "\\begin{align}\n",
        "a \\cdot b = (3)(4) + (7)(1) + (2)(12) + (4)(6) \\\\ \n",
        "= 12 + 7 + 24 + 24  \\\\\n",
        "= 67\n",
        "\\end{align}\n",
        "\n",
        "The dot product is commutative: $ a \\cdot b = b \\cdot a$\n",
        "\n",
        "The dot product is distributive: $a \\cdot (b + c) = a \\cdot b + a \\cdot c$\n",
        "\n",
        "Two vectors must have the same number of components in order for the dot product to exist. If they lengths differ the dot product is undefined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1hxvXtLqRhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_dot_product(vector1, vector2):\n",
        "  assert len(vector1) == len(vector2), 'Expected Vectors of Equal Length'\n",
        "  \n",
        "  return sum([vector1[i]*vector2[i] for i in range(len(vector1))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUeBZtVr-nro",
        "colab_type": "text"
      },
      "source": [
        "### Cross Product\n",
        "\n",
        "The Cross Product is the vector equivalent of multiplication. The result is a third vector that is perpendicular to the first two vectors.\n",
        "\n",
        "It is written with a regular looking multiplication sign like $a \\times b$ but it is read as \"a cross b\"\n",
        "\n",
        "The cross product equation is a little complicated, and gaining an intuition for it is going to take a little bit more time than we have here. I think it's the least useful of the vector operations, but I'll give you a short example anyway.\n",
        "\n",
        "\n",
        "Assume that we have vectors $x$ and $y$.\n",
        "\n",
        "\\begin{align}\n",
        "x = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}\n",
        "\\qquad\n",
        "y = \\begin{bmatrix} y_1 & y_2 & y_3 \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "The cross product can be found by taking these two vectors and adding a third unit vector to create a 3x3 matrix and then finding the determinant of the 3x3 matrix like follows:\n",
        "\n",
        "$$\n",
        "x = \\begin{vmatrix} i & j & k \\\\  x_1 & x_2 & x_3 \\\\  y_1 & y_2 & y_3 \\end{vmatrix} = i\\begin{vmatrix} x_2 & x_3 \\\\ y_2 & y_3 \\end{vmatrix} + j\\begin{vmatrix}x_1 & x_3 \\\\y_1 & y_3\\end{vmatrix} + k\\begin{vmatrix}x_1 & x_2 \\\\y_1 & y_2\\end{vmatrix} = \n",
        "\\\\\n",
        "\\\\\n",
        "i(x_2y_3-x_3y_2) - j(x_1y_3-x_3y_1) + k(x_1y_2-x_2y_1)\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drCVY_F8SaPI",
        "colab_type": "text"
      },
      "source": [
        "## Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKYeFAUbaP4j",
        "colab_type": "text"
      },
      "source": [
        "###Definition\n",
        "A rectangular grid of numbers arranged in rows and columns. Variables that represent matrices are typically written as capital letters (boldfaced as well if you want to be super formal).\n",
        "\n",
        "\\begin{align}\n",
        "A = \n",
        "    \\begin{bmatrix}\n",
        "           1 & 2 & 3\\\\\n",
        "           4 & 5 & 6\\\\\n",
        "           7 & 8 & 9\n",
        "    \\end{bmatrix}\n",
        "    \\qquad\n",
        "    B = \\begin{bmatrix}\n",
        "           1 & 2 & 3\\\\\n",
        "           4 & 5 & 6\n",
        "    \\end{bmatrix}\n",
        " \\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKAHHil-0bm",
        "colab_type": "text"
      },
      "source": [
        "### Dimensionality\n",
        "\n",
        "The number of rows and columns that a matrix has is called its **dimension**.\n",
        "\n",
        "When listing the dimension of a matrix we always list rows first and then columns. \n",
        "\n",
        "The dimension of matrix A is 3x3, and the dimensions of B are 2x3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqWzluoD-9Pf",
        "colab_type": "text"
      },
      "source": [
        "### Matrix Equality\n",
        "\n",
        "In order for two Matrices to be equal the following conditions must be true:\n",
        "\n",
        "1) They must have the same dimensions.\n",
        "\n",
        "2) Corresponding elements must be equal.\n",
        "\n",
        "\\begin{align}\n",
        "    \\begin{bmatrix}\n",
        "           1 & 4\\\\\n",
        "           2 & 5\\\\\n",
        "           3 & 6 \n",
        "    \\end{bmatrix} \n",
        "    \\neq\n",
        "    \\begin{bmatrix}\n",
        "           1 & 2 & 3\\\\\n",
        "           4 & 5 & 6\n",
        "    \\end{bmatrix}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYwxF4R8_Gnf",
        "colab_type": "text"
      },
      "source": [
        "### Matrix Multiplication\n",
        "\n",
        "You can multipy any two matrices where the number of  columns of the first matrix is equal to the number of rows of the second matrix.\n",
        "\n",
        "The unused dimensions of the factor matrices tell you what the dimensions of the product matrix will be.\n",
        "\n",
        "<center>![Matrix Multiplication Dimensions](https://cdn.kastatic.org/googleusercontent/qCWpcd2cPFXad5omuF_ulaYlaBomeBw5RS_9GYLUJg9lZd25SIuvConuu_CfoGBzZJjZGr05602QTJyR0JImBeG39A)</center>\n",
        "\n",
        "There is no commutative property of matrix multiplication (you can't switch the order of the matrices and always get the same result). \n",
        "\n",
        "Matrix multiplication is best understood in terms of the dot product. Remember:\n",
        "\n",
        "\\begin{align} \\vec{a} \\cdot \\vec{b} = (a_{1} \\times b_{1}) + (a_{2} \\times b_{2}) + \\ldots + ( a_{n} \\times b_{n}) \\end{align}\n",
        "\n",
        "To multiply to matrices together, we will take the dot product of each row of the first matrix with each column of the second matrix. The position of the resulting entries will correspond to the row number and column number of the row and column vector that were used to find that scalar. Lets look at an example to make this more clear.\n",
        "\n",
        "<center>![Dot Product Matrix Multiplication](https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg)</center>\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix} \n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "7 & 8 \\\\\n",
        "9 & 10 \\\\\n",
        "11 & 12\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        "(1)(7)+(2)(9)+(3)(11) & (1)(8)+(2)(10)+(3)(12)\\\\\n",
        "(4)(7)+(5)(9)+(6)(11) & (4)(8)+(5)(10)+(6)(12)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "(7)+(18)+(33) & (8)+(20)+(36)\\\\\n",
        "(28)+(45)+(66) & (32)+(50)+(72)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "58 & 64\\\\\n",
        "139 & 154\n",
        "\\end{bmatrix}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uS2thLE1NuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dimension_match(matrix1,matrix2):\n",
        "  return (len(matrix1[0]) == len(matrix2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJt59CS7mOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rectangular_matrix(matrix):\n",
        "  for i in range(len(matrix)):\n",
        "    if len(matrix[i]) != len(matrix[0]):\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viTDPWPcAsml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transpose_matrix(matrix):\n",
        "  return list(map(list, zip(*matrix)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSs71QJDzIwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matrix_multiply(matrix1,matrix2):\n",
        "  assert dimension_match(matrix1,matrix2), 'Matrix1 Columns != Matrix2 Rows'\n",
        "\n",
        "  matrix2 = transpose_matrix(matrix2)\n",
        "  product = [[] for _ in range(len(matrix1))]\n",
        "  \n",
        "  for i in range(len(matrix1)):\n",
        "    for j in range(len(matrix2)):\n",
        "      assert (len(matrix1[i]) == len(matrix2[j])), 'Matrices Must Be Rectangular'\n",
        "      product[i].append(vector_dot_product(matrix1[i],matrix2[j]))\n",
        "  return product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJt-S8zA_Lk9",
        "colab_type": "text"
      },
      "source": [
        "###Transpose\n",
        "\n",
        "A transposed matrix is one whose rows are the columns of the original and whose columns are the rows of the original.\n",
        "\n",
        "Common notation for the transpose of a matrix is to have a capital $T$ superscript or a tick mark:\n",
        "\n",
        "\\begin{align}\n",
        "B^{T}\n",
        "\\qquad\n",
        "B^{\\prime}\n",
        "\\end{align}\n",
        "\n",
        "The first is read \"B transpose\" the second is sometimes read as \"B prime\" but can also be read as \"B transpose\".\n",
        "\n",
        "The transpose of any matrix can be found easily by fixing the elements on the main diagonal and flipping the placement of all other elements across that diagonal.\n",
        "\n",
        "\\begin{align}\n",
        "  B = \n",
        "\\begin{bmatrix}\n",
        "  1 & 2 & 3 \\\\\n",
        "  4 & 5 & 6\n",
        "\\end{bmatrix}\n",
        "  \\qquad\n",
        "  B^{T} = \n",
        "  \\begin{bmatrix}\n",
        "  1 & 4 \\\\\n",
        "  2 & 5 \\\\\n",
        "  3 & 6\n",
        "  \\end{bmatrix}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHBlcWmA_Tl8",
        "colab_type": "text"
      },
      "source": [
        "The square of the norm of a vector is equivalent to the dot product of a vector with itself.\n",
        "\n",
        "The dot product of a vector and itself can be rewritten as that vector times the transpose of itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSfLokfx_T4x",
        "colab_type": "text"
      },
      "source": [
        "### Square Matrices:\n",
        "\n",
        "A square matrix is any matrix that has the same number of rows as columns:\n",
        "\n",
        "\\begin{align}\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "  a_{1,1}\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "B =\n",
        "\\begin{bmatrix}\n",
        "b_{1,1} & b_{1,2} \\\\\n",
        "b_{2,1} & b_{2,2}\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "C =\n",
        "\\begin{bmatrix}\n",
        "c_{1,1} & c_{1,2} & c_{1,3} \\\\\n",
        "c_{2,1} & c_{2,2} & c_{2,3} \\\\\n",
        "c_{3,1} & c_{3,2} & c_{3,3} \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "### Special Kinds of Square Matrices\n",
        "\n",
        "**Diagonal:** Values on the main diagonal, zeroes everywhere else.\n",
        "\n",
        "\\begin{align}\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "a_{1,1} & 0 & 0 \\\\\n",
        "0 & a_{2,2} & 0 \\\\\n",
        "0 & 0 & a_{3,3} \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Upper Triangular:** Values on and above the main diagonal, zeroes everywhere else.\n",
        "\n",
        "\\begin{align}\n",
        "B =\n",
        "\\begin{bmatrix}\n",
        "b_{1,1} & b_{1,2} & b_{1,3} \\\\\n",
        "0 & b_{2,2} & b_{2,3} \\\\\n",
        "0 & 0 & b_{3,3} \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Lower Triangular:** Values on and below the main diagonal, zeroes everywhere else.\n",
        "\n",
        "\\begin{align}\n",
        "C =\n",
        "\\begin{bmatrix}\n",
        "c_{1,1} & 0 & 0 \\\\\n",
        "c_{2,1} & c_{2,2} & 0 \\\\\n",
        "c_{3,1} & c_{3,2} & c_{3,3} \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Identity Matrix:** A diagonal matrix with ones on the main diagonal and zeroes everywhere else. The product of the any square matrix and the identity matrix is the original square matrix $AI == A$. Also, any matrix multiplied by its inverse will give the identity matrix as its product.  $AA^{-1} = I$\n",
        "\n",
        "\\begin{align}\n",
        "D =\n",
        "\\begin{bmatrix}\n",
        "  1\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "E =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "F =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1 \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Symmetric:** The numbers above the main diagonal are mirrored below/across the main diagonal.\n",
        "\n",
        "\\begin{align}\n",
        "G =\n",
        "\\begin{bmatrix}\n",
        "1 & 4 & 5 \\\\\n",
        "4 & 2 & 6 \\\\\n",
        "5 & 6 & 3 \n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwAaji5San4",
        "colab_type": "text"
      },
      "source": [
        "### Determinant\n",
        "\n",
        "The determinant is a property that all square matrices possess and is denoted $det(A)$ or using pipes $|A|$\n",
        "\n",
        "The equation given for finding the determinant of a 2x2 matrix is as follows:\n",
        "\n",
        "\\begin{align}\n",
        "A = \\begin{bmatrix}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "|A| = ad-bc\n",
        "\\end{align}\n",
        "\n",
        "The determinant of larger square matrices recursively by finding the determinats of the smaller matrics that make up the large matrix.\n",
        "\n",
        "For example:\n",
        "\n",
        "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/14f2f2a449d6d152ee71261e47551aa0a31c801e\" width=500></center>\n",
        "\n",
        "The above equation is **very** similar to the equation that we use to find the cross-product of a 3x3 matrix. The only difference is the negative sign in front of the $b$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X63lmisa8SRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def square_matrix(matrix):\n",
        "  for i in range(len(matrix)):\n",
        "    if len(matrix[i]) != len(matrix):\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7JidJgk5fDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matrix_determinant(matrix):\n",
        "  assert square_matrix(matrix), 'Expected Square Matrix'\n",
        "  \n",
        "  #if a 2x2 matrix, solve\n",
        "  if len(matrix) == len(matrix[0]) == 2:\n",
        "    return (((matrix[0][0])*(matrix[1][1]))-((matrix[1][0])*(matrix[0][1])))\n",
        "  \n",
        "  total = 0\n",
        "  \n",
        "  #iterate over every column\n",
        "  for column in range(len(matrix)):\n",
        "    #submatrix of matrix excluding the top row\n",
        "    submatrix = [x[:] for x in matrix][1:]\n",
        "    \n",
        "    #create sub-sub-matrices\n",
        "    for i in range(len(submatrix)): \n",
        "      submatrix[i] = submatrix[i][0:column] + submatrix[i][column+1:] \n",
        "    \n",
        "    #find the sign by raising -1 to either 0 or 1\n",
        "    sign = (-1)**column\n",
        "    \n",
        "    #calculate the sub_determinant by recursion of matrix_determinant\n",
        "    sub_determinant = matrix_determinant(submatrix)\n",
        "    \n",
        "    #add the sub_determinant to the total\n",
        "    total += sign * matrix[0][column] * sub_determinant \n",
        "    \n",
        "  return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd-7IDM2-zcl",
        "colab_type": "text"
      },
      "source": [
        "#### What leads to a 0 determinant?\n",
        " \n",
        " A square matrix that has a determinant of 0 is known as a \"singular\" matrix. One thing that can lead to a matrix having a determinant of 0 is if two rows or columns in the matrix are perfectly collinear. Another way of saying this is that the determinant will be zero if the rows or columns of a matrix are not linearly dependent. \n",
        " \n",
        "One of the most common ways that a matrix can end up having rows that are linearly dependent is if one column a multiple of another column. Lets look at an example:\n",
        "\n",
        "\\begin{align}\n",
        "C =\\begin{bmatrix}\n",
        "  1 & 5 & 2 \\\\\n",
        "  2 & 7 & 4 \\\\\n",
        "  3 & 2 & 6\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Look at the columns of the above matrix, column 3 is exactly double column 1. (could be any multiple or fraction) Think about if you had some measure in a dataset of distance in miles, but then you also wanted to convert its units to feet, so you create another column and multiply the mile measure by 5,280 (Thanks Imperial System). But then you forget to drop one of the columns so you end up with two columns that are linearly dependent which causes the determinant of your dataframe to be 0 and will cause certain algorithms to fail. We'll go deeper into this concept next week (this can cause problems with linear regression) so just know that matrices that have columns that are a multiple or fraction of another column will cause the determinant of that matrix to be 0.\n",
        " \n",
        " For more details about when a matrix is invertible google the \"[Invertible Matrix Theorem](https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem)\" but be prepared for some heavy linear algebra jargon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aGG5d4HSauH",
        "colab_type": "text"
      },
      "source": [
        "## Inverse\n",
        "\n",
        " The inverse is like the reciprocal of the matrix that was used to generate it. Just like $\\frac{1}{8}$ is the reciprocal of 8, $A^{-1}$ acts like the reciprocal of $A$.  The equation for finding the determinant of a 2x2 matrix is as follows:\n",
        " \n",
        " \\begin{align}\n",
        "A = \\begin{bmatrix}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}\n",
        "d & -b\\\\\n",
        "-c & a\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "### What happens if we multiply a matrix by its inverse?\n",
        "\n",
        "The product of a matrix multiplied by its inverse is the identity matrix of the same dimensions as the original matrix. There is no concept of \"matrix division\" in linear algebra, but multiplying a matrix by its inverse is very similar since $8\\times\\frac{1}{8} = 1$. \n",
        "\n",
        "\\begin{align}\n",
        "A^{-1}A = I \n",
        "\\end{align}\n",
        "\n",
        "### Not all matrices are invertible\n",
        "\n",
        "Matrices that are not square are not invertible.\n",
        " \n",
        " A matrix is invertible if and only if its determinant is non-zero. You'll notice that the fraction on the left side of the matrix is $\\frac{1}{det(A)}$.\n",
        " \n",
        " As you know, dividing anything by 0 leads to an undefined quotient. Therefore, if the determinant of a matrix is 0, then the entire inverse becomes undefined. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhHQ7ltEkxRJ",
        "colab_type": "text"
      },
      "source": [
        "#Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eF6horugVIl",
        "colab_type": "text"
      },
      "source": [
        "## Variance\n",
        "\n",
        "Variance is a measure of the spread of numbers in a dataset. Variance is the average of the squared differences from the mean. \n",
        "\n",
        "$\\overline{X}$ is the mean of the dataset.\n",
        "\n",
        "$N$ is the total number of observations.\n",
        "\n",
        "$v$ or variance is sometimes denoted by a lowercase v or $\\sigma^{2}$.\n",
        "\n",
        "\\begin{align}\n",
        "v = \\frac{\\sum{(X_{i} - \\overline{X})^{2}} }{N}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaL-WuDKtKI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean(a_list):\n",
        "  return sum(a_list)/len(a_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJt0feEFlAE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def variance(a_list):\n",
        "  return sum([((entry-mean(a_list))**2) for entry in a_list])/len(a_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kHEYk3Ziaad",
        "colab_type": "text"
      },
      "source": [
        "## Standard Deviation\n",
        "The standard deviation is $\\sqrt(variance)$\n",
        "\n",
        "### So why would we use one over the other?\n",
        "\n",
        "Taking the square root of the variance will put the measurement back in the same units as the mean. The Standard Deviation is the average distance from the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT37ipwrmNCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stdv(a_list):\n",
        "  return (variance(a_list)**.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOFz37RxhX4N",
        "colab_type": "text"
      },
      "source": [
        "## Covariance\n",
        "\n",
        "Covariance is a measure of how changes in one variable are associated with changes in a second variable.\n",
        "\n",
        "<center>![Covariance](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2013/12/g-covariance.gif)</center>\n",
        "\n",
        "\\begin{align}\n",
        "cov = \\frac{\\sum{(X_{i} - \\overline{X})(Y_{i} - \\overline{Y})} }{N-1}\n",
        "\\end{align}\n",
        "\n",
        "### Interpeting Covariance\n",
        "\n",
        "A large positive or negative covariance indicates a strong relationship between two variables. However, you can't necessarily compare covariances between sets of variables that have a different scale, since the covariance values are unbounded, they could take on arbitrarily high or low values. This means that you can't compare the covariances between variables that have a different scale. A variable that has a large scale will always have a higher covariance than a variable with an equally strong relationship, yet smaller scale. This means that we need a way to regularlize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDajsMPwhlZU",
        "colab_type": "text"
      },
      "source": [
        "## The Variance-Covariance Matrix\n",
        "\n",
        "A matrix that compares each variable with every other variable in a dataset and returns the variance values along the main diagonal, and covariance values everywhere else. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaoTvHL_htKw",
        "colab_type": "text"
      },
      "source": [
        "## Correlation Coefficient\n",
        "\n",
        "The measure of spread of a variable is the Standard Deviation. If we divide our covariance values by the product of the standard deviations of the two variables, we'll end up with what's called the Correlation Coefficient. \n",
        "\n",
        "Correlation Coefficients have a fixed range from -1 to +1 with 0 representing no linear relationship between the data. \n",
        "\n",
        "In most use cases the correlation coefficient is an improvement over measures of covariance because:\n",
        "\n",
        "- Covariance can take on practically any number while a correlation is limited: -1 to +1.\n",
        "- Because of it’s numerical limitations, correlation is more useful for determining how strong the relationship is between the two variables.\n",
        "- Correlation does not have units. Covariance always has units\n",
        "- Correlation isn’t affected by changes in the center (i.e. mean) or scale of the variables\n",
        "\n",
        "[Statistics How To - Covariance](https://www.statisticshowto.datasciencecentral.com/covariance/)\n",
        "\n",
        "The correlation coefficient is usually represented by a lower case $r$.\n",
        "\n",
        "\\begin{align}\n",
        "r = \\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvGHXOyLqxUE",
        "colab_type": "text"
      },
      "source": [
        "# Intermediate Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnaCekp1h7iK",
        "colab_type": "text"
      },
      "source": [
        "## Orthogonality\n",
        "\n",
        "Orthogonality is another word for \"perpendicularity\". Two vectors or matrices that are perpendicular to one another are orthogonal.\n",
        "\n",
        "### How to tell if two vectors are orthogonal\n",
        "\n",
        "Two vectors are orthogonal to each other if their dot product is zero. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCV78PpNwB8",
        "colab_type": "text"
      },
      "source": [
        "## Unit Vectors\n",
        "\n",
        "A unit vector is any vector of \"unit length\" (1). You can turn any non-zero vector into a unit vector by dividing it by its norm (length/magnitude). Unit vectors are denoted by the hat symbol \"^\" above the variable.\n",
        "\n",
        "Example:\n",
        "\n",
        "\\begin{align}\n",
        " b = \\begin{bmatrix} 1 \\\\ 2 \\\\  2 \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Find the norm\n",
        " \n",
        " \\begin{align}\n",
        " ||b|| = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3\n",
        "\\end{align}\n",
        "\n",
        "Turn b into a unit vector by dividing it by its norm.\n",
        "\n",
        " \\begin{align}\n",
        " \\hat{b} = \\frac{1}{||b||}b = \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{2}{3} \\\\  \\frac{2}{3} \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "You might frequently see mentioned the unit vectors used to denote a certain dimensional space. \n",
        "\n",
        "$\\mathbb{R}$ unit vector: $\\hat{i} = \\begin{bmatrix} 1 \\end{bmatrix}$\n",
        "\n",
        "\n",
        "$\\mathbb{R}^2$ unit vectors: $\\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "$\\mathbb{R}^3$ unit vectors: $\\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$,  $\\hat{k} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "You'll notice that in the corresponding space, these basis vectors are the rows/columns of the identity matrix.\n",
        "\n",
        "### Vectors as linear combinations of scalars and unit vectors\n",
        "\n",
        "Any vector (or matrix) can be be described in terms of a linear combination of scaled unit vectors. \n",
        "\n",
        "Example:\n",
        "\n",
        "\\begin{align}\n",
        "c = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} = 2\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 3\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 2\\hat{i} + 3\\hat{j}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiYx3xqgO3gR",
        "colab_type": "text"
      },
      "source": [
        "## Span\n",
        "\n",
        "The span is the set of all possible vectors that can be created with a linear combination of vectors. \n",
        "\n",
        "## Linearly Dependent Vectors\n",
        "\n",
        "Two or more vectors that live on the same line are linearly dependent. This means that there is no linear combination that will create a vector that lies outside of that line. In this case, the span of these vectors is the line that they lie on.\n",
        "\n",
        "## Linearly Independent Vectors\n",
        "\n",
        "Linearly independent vectors are vectors that don't lie on the same line as each other. If two vectors are linearly independent, then there ought to be some linear combination of them that could represent any vector in the space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRQb0BFqO44V",
        "colab_type": "text"
      },
      "source": [
        "## Basis\n",
        "\n",
        "The basis of a vector space $V$ is a set of vectors that are linearly independent and that span the vector space $V$. A set of vectors spans a space if their linear combinations fill the space.\n",
        "\n",
        "## Orthogonal Basis\n",
        "\n",
        "An orthogonal basis is a set of vectors that are linearly independent, span the vector space, and are orthogonal to each other.\n",
        "\n",
        "## Orthonormal Basis\n",
        "\n",
        "An orthonormal basis is a set of vectors that are linearly independent, span the vector space, are orthogonal to eachother and each have unit length. \n",
        "\n",
        "The unit vectors form an orthonormal basis for whatever vector space that they are spanning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyyqxO8DO7TP",
        "colab_type": "text"
      },
      "source": [
        "## Rank\n",
        "\n",
        "The rank of a matrix is the dimension of the vector space spanned by its columns. Just because a matrix has a certain number of rows or columns (dimensionality) doesn't neccessarily mean that it will span that dimensional space. Sometimes there exists a sort of redundancy within the rows/columns of a matrix (linear dependence) that becomes apparent when we reduce a matrix to row-echelon form via Gaussian Elimination.\n",
        "\n",
        "## Gaussian Elimination \n",
        "\n",
        "Gaussian Elimination is a process that seeks to take any given matrix and reduce it down to what is called \"Row-Echelon form.\" A matrix is in Row-Echelon form when it has a 1 as its leading entry (furthest left) in each row, and zeroes at every position below that main entry. These matrices will usually wind up as a sort of upper-triangular matrix (not necessarly square) with ones on the main diagonal. \n",
        "\n",
        "<center>![row-echelon form](http://www.mathwords.com/r/r_assets/r61.gif)</center>\n",
        "\n",
        "Gaussian Elimination takes a matrix and converts it to row-echelon form by doing combinations of three different row operations:\n",
        "\n",
        "1) You can swap any two rows\n",
        "\n",
        "2) You can multiply entire rows by scalars\n",
        "\n",
        "3) You can add/subtract rows from each other\n",
        "\n",
        "Example:\n",
        "\n",
        "\\begin{align}\n",
        " P = \\begin{bmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  -2 & -3 & 1 \\\\\n",
        "  3 & 3 & 0 \n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Step 1: Add 2 times the 1st row to the 2nd row\n",
        "\n",
        "\\begin{align}\n",
        " P = \\begin{bmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & -3 & 3 \\\\\n",
        "  3 & 3 & 0 \n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Step 2: Add -3 times the 1st row to the 3rd row\n",
        "\n",
        "\\begin{align}\n",
        " P = \\begin{bmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & -3 & 3 \\\\\n",
        "  0 & 3 & -3 \n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Step 3: Multiply the 2nd row by -1/3\n",
        "\n",
        "\\begin{align}\n",
        " P = \\begin{bmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & 1 & -1 \\\\\n",
        "  0 & 3 & -3 \n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Step 4: Add -3 times the 2nd row to the 3rd row\n",
        "\n",
        "\\begin{align}\n",
        " P = \\begin{bmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & 1 & -1 \\\\\n",
        "  0 & 0 & 0 \n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Because we have two rows with leading 1s (these are called **pivot values**) left after the matrix is in row-echelon form, we know that its Rank is 2. \n",
        "\n",
        "This means that even though the original matrix is a 3x3 matrix, it can't span $\\mathbb{R}^3$, only $\\mathbb{R}^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRP9f3AElTPl",
        "colab_type": "text"
      },
      "source": [
        "## Projection\n",
        "The projection to a subspace is the place that is closest to the original point. Geometry tells us that we can find this closest point by dropping a perpendicular line from the point to the space.\n",
        "\n",
        "### Notation\n",
        "In linear algebra we write the projection of w onto L like this: \n",
        "\n",
        "\\begin{align}proj_{L}(\\vec{w})\\end{align}\n",
        "\n",
        "Equation for the projection of any vector $w$ onto a line $L$:\n",
        "\\begin{align}\n",
        "proj_{L}(w) =  \\frac{w \\cdot v}{v \\cdot v}v\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMDvENbV7WGi",
        "colab_type": "code",
        "outputId": "eca9d88e-8824-49e1-b202-220775e395f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Axis Bounds\n",
        "plt.xlim(-1.1,4)          \n",
        "plt.ylim(-1.1,4)\n",
        "\n",
        "# Original Vector\n",
        "v = [1,1/2] \n",
        "w = [2,2]\n",
        "proj = np.multiply(2.4,v)\n",
        "\n",
        "# Set axes\n",
        "axes = plt.gca()\n",
        "axes.set_aspect('equal')\n",
        "\n",
        "# Get Vals for L\n",
        "x_vals = np.array(axes.get_xlim())\n",
        "y_vals = 1/2*x_vals\n",
        "\n",
        "# Plot Vectors and L\n",
        "plt.plot(x_vals, y_vals, '--', color='b', linewidth=1)\n",
        "plt.arrow(0, 0, proj[0], proj[1], linewidth=3, head_width=.05, head_length=0.05, color ='gray')\n",
        "plt.arrow(0, 0, v[0], v[1], linewidth=3, head_width=.05, head_length=0.05, color ='green')\n",
        "plt.arrow(0, 0, w[0], w[1], linewidth=3, head_width=.05, head_length=0.05, color ='red')\n",
        "\n",
        "plt.title(\"Projection of Red onto Green\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyvcyhEp-X79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def projection(w,v):\n",
        "  return np.multiply((np.dot(w,v)/np.dot(v,v)),v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w94ysjXy09pt",
        "colab_type": "text"
      },
      "source": [
        "#Vector Transformations and Dimensionality Reduction\n",
        "\n",
        "**Linear Transformation** - A mapping $V \\rightarrow W$ between two modules (including vector spaces) that preserves the operations of addition and scalar multiplication. Requirements:\n",
        "1.   T(u + v) = T(u) + T(v)\n",
        "2.   T(cu) = cT(u)\n",
        "\n",
        "**Eigenvector** - Any vector that doesn't change its orientation during a given transformation. An eigenvector may still be scaled by a scalar.\n",
        "\n",
        "**Eigenvalue** - The scalar that represents how a corresponding eigenvector was scaled during a transformation. Eigenvectors and eigenvalues always come in pairs.\n",
        "\n",
        "**[Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)** - A term that is used to refer to some of the challenges and limitations that arise from trying to process or model datasets with a large number of features (often hundreds or thousands). When the dimensionality increases, the volume of the space increases so fast that the available data become sparse, requiring more data to determine statistical signifigance or find relationships, while increasing the computational load."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-NIz65s7YO6",
        "colab_type": "text"
      },
      "source": [
        "##Dimensionality Reduction Techniques\n",
        "\n",
        "**Feature Selection** - Selecting a subset of the most influential features.\n",
        "\n",
        "**Feature Extraction** - Create a new, smaller subset that captures the most influential information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vse1PlIy8FYd",
        "colab_type": "text"
      },
      "source": [
        "##Principle Component Analysis (PCA)\n",
        "\n",
        "**Principle Component Analysis** - A feature extraction technique that transforms a high dimensional dataset into a new lower dimensional dataset while preserving the maximum amount of information from the original data.\n",
        "\n",
        "**PCA Process**\n",
        "1.   Separate the data into X (features) and y (target) variables\n",
        "2.   Center each column at 0 by subtracting its mean\n",
        "3.   Divide each column by its standard deviation to get Z\n",
        "4.   Calculate the variance-covariance matrix of Z\n",
        "5.   Calculate the eigenvectors and eigenvalues of the variance-covariance matrix\n",
        "6.   Sort the eigenvalue and eigenvector pairs\n",
        "7.   Use a matrix transformation to project datapoints onto our eigenvector subspace\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xASVwnZ01jBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.prepocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df)\n",
        "scaled = pd.DataFrame(scaler.transform(df))\n",
        "pca = PCA(2) #number of Principle Components to reduce to\n",
        "pca.fit(scaled)\n",
        "pca_df = pd.DataFrame(pca.transform(scaled), columns=['PC 1', 'PC 2'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBXgPTRE_Bq6",
        "colab_type": "text"
      },
      "source": [
        "#Clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fXamuUx_G2N",
        "colab_type": "text"
      },
      "source": [
        "Machine Learning Overview\n",
        "- **Supervised**: Labelled outputs\n",
        "    - Classification: Discrete output cagetories\n",
        "    - Regression: Continuous output values\n",
        "- **Unsupervised**: Outputs are not labelled\n",
        "- **Reinforcement**: Rewards/punishments for \"behaviors\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgIwqAc_pJS",
        "colab_type": "text"
      },
      "source": [
        "**Clustering** - The assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense. Clustering is a method of unsupervised learning. Process for clustering given a set of points in n-dimensional space:\n",
        "\n",
        "1. Select k random points to act as initial centroids (one point for each cluster)\n",
        "\n",
        "2. Find the cluster of points surrounding that centroid (assign points to the centroid that they lie closest to)\n",
        "\n",
        "3. Calculate a new centroid for the cluster\n",
        "\n",
        "4. Repeat steps 2 & 3 until the model converges\n",
        "\n",
        "**K-Means Clustering** - An algorithm to find groups in the data, with the number of groups represented by the variable K, with the center of each group being the mean. Selecting the number of groups can be done visually through the use of an elbow graph and looking for where the slope decreases the most.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgtqeVyQE6Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train and fit a model with n_clusters\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "kmeans.fit(df)\n",
        "\n",
        "#add a new column with the cluster labels\n",
        "df['clusters'] = kmeans.labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glwXUEOMCi6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Elbow graph\n",
        "distortions = []\n",
        "k = range(1,10)\n",
        "#fit a model from 1 to 10 clusters, and plot it to view inflections\n",
        "for i in k:\n",
        "  k_model = KMeans(n_clusters=i).fit(points)\n",
        "  distortions.append(k_model.inertia_)\n",
        "  \n",
        "plt.plot(k,distortions)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwVHq5zfEa0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting clusters function from lecture, why reinvent the wheel\n",
        "def plot_clusters(df, column_header, centroids):\n",
        "  colors = {0:'red', 1:'green', 2:'blue'}\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.title('k-means clustering')\n",
        "  \n",
        "  ax.plot(centroids.iloc[0].x, centroids.iloc[0].y, \"ok\")\n",
        "  ax.plot(centroids.iloc[1].x, centroids.iloc[1].y, \"ok\")\n",
        "  ax.plot(centroids.iloc[2].x, centroids.iloc[2].y, \"ok\")\n",
        "  \n",
        "  grouped = df.groupby(column_header)\n",
        "  for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', \n",
        "               label=key, color=colors[key])\n",
        "  plt.show()\n",
        "\n",
        "def get_centroids(df,column):\n",
        "  return df.groupby(column).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvnbNhwzDk7q",
        "colab_type": "text"
      },
      "source": [
        "#No Free Lunch\n",
        "\n",
        "The no free lunch principle states that the more an algorithm is optimized to solve one specific kind of problem, the worse it gets at solving all other kinds of problems.\n",
        "\n",
        "1) There are always tradeoffs when selecting from different approaches. Understanding those tradeoffs and justifying the choice of methodology is just as important as actually doing the work itself.\n",
        "\n",
        "2) The only way that we can choose one approach over another is to make assumptions about the data. If we don't know anything about the characteristics of the data, then we can't make an informed choice of algorithm."
      ]
    }
  ]
}