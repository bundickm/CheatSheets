{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Validation Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/Classification_Validation_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cNTOTWvChtp",
        "colab_type": "text"
      },
      "source": [
        "#Resources\n",
        "\n",
        "[Cheat Sheet Repo](https://github.com/bundickm/CheatSheets)\n",
        "\n",
        "[List of Classification and Regression Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)\n",
        "\n",
        "[Yellowbrick: Machine Learning Visualization](https://www.scikit-yb.org/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxZDS6WqPuF4",
        "colab_type": "text"
      },
      "source": [
        "#Baselines and Validation\n",
        "[**Model Validation**](https://github.com/bundickm/DS-Unit-2-Sprint-3-Classification-Validation/blob/master/module2-baselines-validation/model-validation-preread.md) - The process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.\n",
        "\n",
        "Some Model Validation Methods\n",
        "- Performance estimation\n",
        " - 2-way holdout method (train/test split)\n",
        " - (Repeated) k-fold cross-validation without independent test set\n",
        "- Model selection (hyperparameter optimization) and performance estimation ← *We usually want to do this*\n",
        " - 3-way holdout method (train/validation/test split)\n",
        " - (Repeated) k-fold cross-validation with independent test set\n",
        "\n",
        "\"There is a pair of ideas that you must understand in order to do inference correctly:\n",
        "```\n",
        "        Each observation can either be used for exploration or confirmation, not both.\n",
        "\n",
        "        You can use an observation as many times as you like for exploration, \n",
        "        but you can only use it once for confirmation.As soon as you use an \n",
        "        observation twice, you’ve switched from confirmation to exploration.\n",
        "```\n",
        "This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.\"\n",
        "\n",
        "[**Baselines**](https://github.com/bundickm/DS-Unit-2-Sprint-3-Classification-Validation/blob/master/module2-baselines-validation/model-validation-preread.md#why-begin-with-baselines) - A baseline is a very basic model/solution to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against. Why start with a baseline? It will take you less than 1/10th of the time, and could provide up to 90% of the results as well as put a more complex model into context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcQOIrvcWlhI",
        "colab_type": "text"
      },
      "source": [
        "**Leaky Data** - Any feature whose value would not actually be available in practice at the time you'ld want to use the model to make a prediction (ex. - Data from the future, that you would only get after the event occured). An indicator of leaky data are results that are \"too good to be true.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFqbQcewXMuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#Shallow tree can work as a quick baseline\n",
        "tree = DecisionTreeClassifier(max_depth=2)\n",
        "tree.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COktN-3IXVf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "#visualizing the tree can expose possible leaky data\n",
        "dot_data = export_graphviz(tree, out_file=None, \n",
        "                           feature_names=X_train_numeric.columns, \n",
        "                           class_names=['No', 'Yes'], filled=True, \n",
        "                           impurity=False, proportion=True)\n",
        "\n",
        "graphviz.Source(dot_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YT3QlrmC476",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression\n",
        "**Logistic Regression** - a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\n",
        "\n",
        "The goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest (dependent variable) and a set of independent (predictor or explanatory) variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws9CUc2tCaBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#read in the data\n",
        "df = pd.read_csv('some.csv')\n",
        "\n",
        "#split the data\n",
        "target = 'target_feature'\n",
        "X = df.drop(target)\n",
        "y = df[target]\n",
        "\n",
        "#train-test-split or some other validation split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                    test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "#instantiate and fit the model\n",
        "log_reg = LogisticRegression(solver='lbfgs')\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "#make predictions\n",
        "y_pred = log_reg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koohXBAKGFUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Calculate Accuracy (correct predictions/total predictions)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAZ20-ZGf7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#10 splits with the test data and calculate the models accuracy on each\n",
        "scores = cross_val_score(log_reg, X_test, y_test, cv=10, scoring='accuracy')\n",
        "\n",
        "print('Cross-Validation Accuracy Scores', scores)\n",
        "scores.min(), scores.mean(), scores.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRWbYZLGooG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#see the coefficients of the model\n",
        "coefficients = pd.Series(log_reg.coef_[0], X_train.columns)\n",
        "coefficients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NzR0AI3IR7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#see the predicted probabilities instead of the binary predictions\n",
        "log_reg.predict_proba(test_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1apTvdP068Q",
        "colab_type": "text"
      },
      "source": [
        "#Radom Forests and Gradient Boosting\n",
        "\n",
        "**Ensemble Methods** - Machine learning techniques that combines several base models in order to produce one optimal predictive model.\n",
        "\n",
        "**Bagging (Bootstrap Aggregating)** - A machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
        "\n",
        "<center><img src=\"https://victorzhou.com/media/random-forest-post/random-forest.svg\" width=\"400\"/></center>\n",
        "\n",
        "**Random Forest** - An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "1. Each tree trains on a random bootstrap sample (sample with replacement) of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the bootstrap parameter's default is `True`.) This type of ensembling is called Bagging.\n",
        "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rBZPuL25bih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=4, \n",
        "                                class_weight={1:5,0:1})\n",
        "forest.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3HKzMn5b1f",
        "colab_type": "text"
      },
      "source": [
        "**Boosting** - Boosting works in a similar way to bagging, except that the models are made sequentially: each model is grown using information from previously made models. Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision model to the residuals from the current model. We then add this new model into the fitted function in order to update the residuals. By fitting models to the residuals, we slowly improve fˆ in areas where it does not perform well. Note that in boosting, unlike in bagging, the construction of each model depends strongly on the models that have already been grown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK_07Mz-6eAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "booster = XGBClassifier(n_estimators=20, n_jobs=-1)\n",
        "booster.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK0hAsJSYcO9",
        "colab_type": "text"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O2wHh-VdpeO",
        "colab_type": "text"
      },
      "source": [
        "##Classification Metrics and Confusion Matrix\n",
        "\n",
        "**Accuracy** - The proportion of predictions a model got correct. \\begin{align}Accuracy = \\frac{\\text{True Positives + True Negatives}}{\\text{Total Number of Predictions}}\\end{align}\n",
        "\n",
        "**Precision** - The proportion of positive identifications that were actually correct. \"How useful the results are\" \\begin{align}Precision = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\\end{align}\n",
        "\n",
        "**Recall** - The proportion of actual positives that were correctly identified.  \"How complete the results are\" \\begin{align}Recall = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\end{align}\n",
        "\n",
        "**F1 Score** -  The harmonic average of the precision and recall; an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. \n",
        "\\begin{align}F1 = 2\\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}\\end{align}\n",
        "\n",
        "Precision and recall are measuring the relevance of results and are intertwined. Generally, a positive increase in one will result in a negative increase in the other. In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.The F1 score attempts to measure how well precision and recall are balanced.\n",
        "\n",
        "**Confusion Matrix (error matrix)** - A table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEgqWeognBgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#The classification_report will give precision, recall, and F1\n",
        "print(classification_report(y, y_pred))\n",
        "\n",
        "#Or we can calculate the metrics from the confusion matrix\n",
        "pd.DataFrame(confusion_matrix(y, y_pred), \n",
        "             columns=['Predicted Negative', 'Predicted Positive'], \n",
        "             index=['Actual Negative', 'Actual Positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiB9wFZsdwkz",
        "colab_type": "text"
      },
      "source": [
        "##ROC-AUC\n",
        "\n",
        "**Discrimination Threshold** - The probability or score at which the positive class is chosen over the negative class. Generally, this is set to 50% but the threshold can be adjusted to increase or decrease the sensitivity to false positives or to other application factors.\n",
        "\n",
        "**Receiver Operating Characteristic (ROC) Curve** - A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. In simple terms, it illustrates the price you pay in terms of false positive rate to increase the true positive rate. The conservatism is controlled via thresholds on confidence scores to assign the positive and negative label.\n",
        "\n",
        "**[ROC-AUC](https://www.kaggle.com/learn-forum/53782)** -  The area under the ROC curve, measures how well a classifier ranks predicted probabilities. It ranges from 0 to 1. The ROC-AUC is good for classifications with a class imbalance since a naive majority class baseline will have an ROC-AUC score of 0.5. AUC scoring also allows us to evaluate models independently of the threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ0f_zigv4ZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualize the ROC curve\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "#get the false-positive and true positives\n",
        "fpr, tpr, thresholds = roc_curve(y, y_pred_proba)\n",
        "\n",
        "#plot it\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "print('Area under the Receiver Operating Characteristic curve:', \n",
        "      roc_auc_score(y, y_pred_proba))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOmbQusZbOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adjusting threshold for accuracy\n",
        "pipe.fit(X,y)\n",
        "((pd.Series(pipe.predict_proba(X)[:,1])>.65) == y).value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZohzHQH9IVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adjusting threshold for precision\n",
        "pipe.fit(X,y)\n",
        "true_pos = ((pd.Series(pipe.predict_proba(X)[:,1])>.1) & y).value_counts()[1]\n",
        "pred_pos = (pd.Series(pipe.predict_proba(X)[:,1])>.1).value_counts()[1]\n",
        "print('Precision:',true_pos/pred_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlw-SPky9M0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adjusting threshold for recall\n",
        "pipe.fit(X,y)\n",
        "true_pos = ((pd.Series(pipe.predict_proba(X)[:,1])>.1) & y).value_counts()[1]\n",
        "actual_pos = y.value_counts()[1]\n",
        "print('Recall:',true_pos/actual_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vXTFfHywx9-",
        "colab_type": "text"
      },
      "source": [
        "##Imbalanced Classes\n",
        "\n",
        "**[Imbalanced Classes](https://www.svds.com/tbt-learning-imbalanced-classes/)** - Target classes with unequal distribution (More `True` than `False`, more `Red` than `Blue` or `Yellow`). Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration2. In the worst case, minority examples are treated as outliers of the majority class and ignored. The learning algorithm simply generates a trivial classifier that classifies every example as the majority class.\n",
        "\n",
        "A rough outline of useful approaches. Approximately in order of effort:\n",
        "- Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.\n",
        "- Balance the training set in some way:\n",
        " - Oversample the minority class.\n",
        " - Undersample the majority class.\n",
        " - Synthesize new minority classes.\n",
        "- Throw away minority examples and switch to an anomaly detection framework.\n",
        "- At the algorithm level, or after it:\n",
        " - *Adjust the class weight* (misclassification costs).\n",
        " - *Adjust the decision threshold.*\n",
        " - Modify an existing algorithm to be more sensitive to rare classes.\n",
        "- Construct an entirely new algorithm to perform well on imbalanced data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGbIvOghJKaz",
        "colab_type": "text"
      },
      "source": [
        "#Feature Engineering and Misc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qweHGLZJJT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "#one hot encode all string columns\n",
        "encoder = ce.OneHotEncoder.(use_cat_names=True)\n",
        "encoder.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbnQGqgpLMYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "#encode all string columns\n",
        "encoder = ce.OrdinalEncoder()\n",
        "encoder.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu0vjMYpJXiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#fill in missing values\n",
        "imputer.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KuVLCb3Mdin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#visualize the coefficients\n",
        "coefficients = pd.Series(log_reg.coef_[0], X_train.columns)\n",
        "plt.figure(figsize=(10,10))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG__mAuaMuyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Standardize the data (center on the mean and set unit variance)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKfwkPvgNUZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#set all features to have a range of 0 to 1\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D_f7RsxOjZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#example pipeline, all parts in pipe must follow sklearn format\n",
        "pipe = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    MinMaxScaler(), \n",
        "    LogisticRegression(solver='lbfgs', max_iter=1000))\n",
        "\n",
        "#End Case 1: Scoring Method\n",
        "scores = cross_val_score(pipe, X, y, cv=10)\n",
        "\n",
        "#End Case 2: Calling pipe just like any other sklearn model\n",
        "pipe.fit(X, y)\n",
        "y_pred = pipe.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwTV9ke07Lrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test multiple models in quick succession, preprocess data before this\n",
        "models = [LogisticRegression(solver='lbfgs', max_iter=1000), \n",
        "          DecisionTreeClassifier(max_depth=3), \n",
        "          DecisionTreeClassifier(max_depth=None), \n",
        "          RandomForestClassifier(max_depth=3, n_estimators=100, n_jobs=-1, \n",
        "                                 random_state=42), \n",
        "          RandomForestClassifier(max_depth=None, n_estimators=100, n_jobs=-1, \n",
        "                                 random_state=42), \n",
        "          XGBClassifier(max_depth=3, n_estimators=100, n_jobs=-1, \n",
        "                        random_state=42)]\n",
        "\n",
        "for model in models:\n",
        "  print(model, '\\n')\n",
        "  score = cross_val_score(model, titanic_X, titanic_y, \n",
        "                          scoring='accuracy', cv=5).mean()\n",
        "  print('Cross-Validation Accuracy:', score, '\\n', '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1MzVxt77OcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualize feature importance for multiple models, pairs with above cell above\n",
        "for model in models:\n",
        "  name = model.__class__.__name__\n",
        "  model.fit(titanic_X, titanic_y)\n",
        "  if name == 'LogisticRegression':\n",
        "    coefficients = pd.Series(model.coef_[0], titanic_X.columns)\n",
        "    coefficients.sort_values().plot.barh(color='grey', title=name)\n",
        "    plt.show()\n",
        "  else:\n",
        "    importances = pd.Series(model.feature_importances_, titanic_X.columns)\n",
        "    title = f'{name}, max_depth={model.max_depth}'\n",
        "    importances.sort_values().plot.barh(color='grey', title=title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}